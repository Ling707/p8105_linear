---
title: "p8105_linear_model"
author: "Ling"
date: "11/16/2021"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(p8105.datasets)
```

# linear model 1/3: linear models

## lecture

- Schedule
  - Bootstrapping + Iteration
- Modeling and testing
  - Linear regression: AR1
  - GLM
  - `lm` for linear models
    - data, y, x
    - clean up the output: `broom` package

## practice
```{r}
set.seed(1)
data("nyc_airbnb")
nyc_airbnb = 
  nyc_airbnb %>% 
  mutate(stars = review_scores_location / 2) %>% 
  rename(
    borough = neighbourhood_group,
    neighborhood = neighbourhood) %>% 
  filter(borough != "Staten Island") 

fit = lm(price ~ stars + borough, data = nyc_airbnb) #%>%
  broom::tidy() %>%
  # versus broom::glance()
  select(term, estimate, p.value) %>% 
  mutate(term = str_replace(term, "^borough", "Borough: ")) %>% 
  knitr::kable(digits = 3)

# residuals
modelr::add_residuals(nyc_airbnb, fit) %>%
  ggplot(aes(x = borough, y = resid)) + geom_violin() # skewed residuals
  # make a residual plot
modelr::add_predictions(nyc_airbnb, fit)
```

- hypothesis testing: type 3 analysis
```{r}
fit_null = lm(price ~ stars + borough, data = nyc_airbnb)
fit_alt = lm(price ~ stars + borough + room_type, data = nyc_airbnb)
```

- nesting data & interaction
```{r}
nyc_airbnb %>% 
  lm(price ~ stars * borough + room_type * borough, data = .) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)

# versus

nest_lm_res =
  nyc_airbnb %>% 
  nest(data = -borough) %>% 
  mutate(
    models = map(data, ~lm(price ~ stars + room_type, data = .x)), # notice the '.x'
    results = map(models, broom::tidy)) %>% 
  select(-data, -models) %>% 
  unnest(results)

# setup regression model based on different borough

# restrict to manhattan

manhattan_nest_lm_res =
  nyc_airbnb %>% 
  filter(borough == "Manhattan") %>% 
  nest(data = -neighborhood) %>% 
  mutate(
    models = map(data, ~lm(price ~ stars + room_type, data = .x)),
    results = map(models, broom::tidy)) %>% 
  select(-data, -models) %>% 
  unnest

manhattan_nest_lm_res %>% 
  filter(str_detect(term, "room_type")) %>% 
  ggplot(aes(x = neighborhood, y = estimate)) + 
  geom_point() + 
  facet_wrap(~term) + 
  theme(axis.text.x = element_text(angle = 80, hjust = 1))
```

- mix model: random intercept and slope

```{r}
manhattan_airbnb %>% 
  lme4::lmer(price ~ stars + room_type + (1 + room_type | neighborhood), data = .) %>% #neighborhood as random?
  broom.mixed::tidy()
```

- logistic reg: using `glm` function
```{r}
fit_logistic = 
  baltimore_df %>% 
  glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) 
# notice the family: describing the distribution

# tidying logistic reg output
baltimore_df %>% 
  modelr::add_predictions(fit_logistic) %>% 
  mutate(fitted_prob = boot::inv.logit(pred))

# R assumes the logit is important, so "inverse logit" is needed (`inv.logit`)
```

# 2/3 cross validation
## lecture

- flexibility vs. fit: balance
- prediction accuracy
- cross validation: training set and testing set
  - parameter: RMSE (root mean squared error)
- refinements and variations
- `modelr`
  - `add_predictions()`
  - `add_residuals()`
  - `rmse()`
  - `crossv_mc()`
- `mgcv`

## practice
- cv by hand: `rmse`
- CV iteratively
  - `crossv_mc()`
  - piece-wise linear model
  
# 3/3 Bootstrapping
## lecture

- extra topics
  - shiny
  - simulation/stat
- Repeated sampling
  - CLT
  - repeated sampling on a computer --> bootstrapping (mimic repeated sampling)
- Bootstrapping: repeated random sampling with replacement
  - Pro: get the mean without assumption/large sample size
  - replicate the thought process and draw out the real distribution --> correct claims of parameter distribution, not the p-values
  - `modelr::bootstrap`
- DDx
  - Bootstrap vs. cross-validation
    - Bootstrapping: inference on certain parameter
    - Cross-validation: how well does the model work
  - Bootstrap vs. Jackknife
    - similar?

## practice

- Setup a simulation data set: n =250, linear relationship, x and error are nl distributed.
```{r}
n_samp = 250

sim_df_const = 
  tibble(
    x = rnorm(n_samp, 1, 1),
    error = rnorm(n_samp, 0, 1),
    y = 2 + 3 * x + error
  )

# non-constant variance data, notice the error.
sim_df_nonconst = sim_df_const %>% 
  mutate(
  error = error * .75 * x,
  y = 2 + 3 * x + error
)

sim_df = 
  bind_rows(const = sim_df_const, nonconst = sim_df_nonconst, .id = "data_source") 

# const data vs. nonconst data
sim_df %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = .5) +
  stat_smooth(method = "lm") +
  facet_grid(~data_source) 

# how linear regression model fits the 2 data?
lm(y ~ x, data = sim_df_const) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)

lm(y ~ x, data = sim_df_nonconst) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)
```

- get a bootstrap sample
```{r}
# first, write a function to bootstrap
boot_sample = function(df) {
  sample_frac(df, replace = TRUE, size = 1)
}

boot_sample(sim_df_nonconst) %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = .5) +
  stat_smooth

boot_straps = 
  tibble(
    strap_number = 1:1000,
    strap_sample = rerun(1000, boot_sample(sim_df_nonconst)) # note the rerun function
  )

bootstrap_results = boot_straps %>% 
  mutate(
    model = map(.x = strap_sample, ~lm(y ~ x, data = .x)),
    result = map(model, broom::tidy)
  ) %>% 
  select(-strap_sample, -model) %>% 
  unnest(result)

lm(y~x, data = sim_df_nonconst) %>%
  broom::tidy()
# versus
bootstrap_results %>% 
  group_by(term) %>% 
  summarize(boot_se = sd(estimate)) %>% 
  knitr::kable(digits = 3)

# get the 95% CI under bootstrappling
bootstrap_results %>% 
  group_by(term) %>% 
  summarize(
    ci_lower = quantile(estimate, 0.025), 
    ci_upper = quantile(estimate, 0.975))
```

- `modelr::bootstrap`
```{r}
boot_straps = 
  sim_df_nonconst %>% 
  modelr::bootstrap(n = 1000)

boot_straps$strap[[1]] %>%
  as_tibble()

# make the lines less using modelr::bootstrap
sim_df_nonconst %>% 
  modelr::bootstrap(n = 1000) %>% #note this new line
  mutate(
    models = map(strap, ~lm(y ~ x, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  group_by(term) %>% 
  summarize(boot_se = sd(estimate))
```

- e.g. using air BNB data
```{r}
data("nyc_airbnb")

nyc_airbnb = 
  nyc_airbnb %>% 
  mutate(stars = review_scores_location / 2) %>%
  rename(
    boro = neighbourhood_group,
    neighborhood = neighbourhood) %>% 
  filter(boro != "Staten Island") %>%
  select(price, stars, boro, neighborhood, room_type)

nyc_airbnb %>% 
  ggplot(aes(x = stars, y = price, color = room_type)) + 
  geom_point()
# lots outliers

# try manhattan using bootstrapping
nyc_airbnb %>% 
  filter(boro == "Manhattan") %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~ lm(price ~ stars + room_type, data = .x)),
    results = map(models, broom::tidy)) %>% 
  select(results) %>% 
  unnest(results) %>% 
  filter(term == "stars") %>% 
  ggplot(aes(x = estimate)) + geom_density()

# notice the plot shows the distribution
```


